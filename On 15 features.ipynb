{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5be507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in the CSV file: 186\n",
      "Number of columns in the CSV file: 15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df1 = pd.read_csv('normalized_dataset.csv')\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = len(df1.columns)\n",
    "\n",
    "print(\"Number of columns in the CSV file:\", num_columns)\n",
    "\n",
    "# Read the CSV file\n",
    "df2 = pd.read_csv('large-mergedDataset.csv')\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = len(df2.columns)\n",
    "\n",
    "print(\"Number of columns in the CSV file:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7125dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged DataFrame: (19538, 200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Merge the DataFrames based on 'id' column\n",
    "merged_df = pd.merge(df1, df2, on='id')\n",
    "\n",
    "# Print the shape of the merged DataFrame to confirm the merge was successful\n",
    "print(\"Shape of merged DataFrame:\", merged_df.shape)\n",
    "\n",
    "merged_df.to_csv('merged_dataset.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f1c6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in the CSV file: 200\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df3 = pd.read_csv('merged_dataset.csv')\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = len(df3.columns)\n",
    "\n",
    "print(\"Number of columns in the CSV file:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2bd903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of preprocessed DataFrame: (19538, 200)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Check if the text is NaN (handles missing values)\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove URLs and specific patterns (if any)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'www\\S+', '', text)\n",
    "\n",
    "        # Remove punctuation and non-word characters\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        # Lemmatize the words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        # Join tokens back to a cleaned text\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "    else:\n",
    "        cleaned_text = ''  # Replace NaN with an empty string\n",
    "    return cleaned_text\n",
    "\n",
    "# Preprocess 'targetTitle' column and update it in the DataFrame\n",
    "df3['targetTitle'] = df3['targetTitle'].apply(preprocess_text)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df3.to_csv('preprocess_merged.csv', index=False)\n",
    "\n",
    "# Print the shape of the preprocessed DataFrame\n",
    "print(\"Shape of preprocessed DataFrame:\", df3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29787e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame with GloVe embeddings: (19538, 201)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the DataFrame with the preprocessed 'targetTitle' column\n",
    "df4 = pd.read_csv('preprocess_merged.csv')\n",
    "\n",
    "# Load GloVe word vectors\n",
    "glove_path = 'glove.6B.100d.txt'\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n",
    "\n",
    "# Function to get GloVe word embeddings\n",
    "def get_glove_embeddings(text):\n",
    "    words = text.split()\n",
    "    embeddings = [glove_model[word] if word in glove_model else np.zeros(100) for word in words]\n",
    "    if not embeddings:\n",
    "        return np.zeros(100)  # Assign a default embedding for empty lists (out-of-vocabulary words)\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Apply GloVe embeddings to the preprocessed 'targetTitle' column\n",
    "df4['targetTitle_glove'] = df3['targetTitle'].apply(get_glove_embeddings)\n",
    "\n",
    "# Save the DataFrame with embeddings to a new CSV file\n",
    "df4.to_csv('preprocess_merged_with_glove.csv', index=False)\n",
    "\n",
    "# Print the shape of the DataFrame with GloVe embeddings\n",
    "print(\"Shape of DataFrame with GloVe embeddings:\", df4.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa66286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocess_merged.csv file\n",
    "df5 = pd.read_csv('preprocess_merged_with_glove.csv')\n",
    "\n",
    "# Separate features (X) and class labels (Y)\n",
    "X = df5[\"targetTitle_glove\"]\n",
    "Y = df5[\"truthClass\"]\n",
    "ids = df5[\"id\"]\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "classes_list = [\"no-clickbait\",\"clickbait\"]\n",
    "label_index = Y.apply(classes_list.index)\n",
    "label1 = np.asarray(label_index)\n",
    "label = to_categorical(np.asarray(label1))\n",
    "y=label1\n",
    "y.shape\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0cf189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Perform train-test split on X, Y, and ids\n",
    "X_temp, X_test, y_temp, y_test, ids_temp, ids_test = train_test_split(X, Y, ids, test_size=0.2, random_state=42)\n",
    "X_dev, X_train, y_dev, y_train, ids_dev, ids_train = train_test_split(X_temp, y_temp, ids_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create DataFrames for X_train, X_dev, and X_test including 'id'\n",
    "X_train_df = df5[df5[\"id\"].isin(ids_train)]\n",
    "X_dev_df = df5[df5[\"id\"].isin(ids_dev)]\n",
    "X_test_df = df5[df5[\"id\"].isin(ids_test)]\n",
    "\n",
    "# Create DataFrames for y_train, y_dev, and y_test including 'id'\n",
    "y_train_df = df5[df5[\"id\"].isin(ids_train)]\n",
    "y_dev_df = df5[df5[\"id\"].isin(ids_dev)]\n",
    "y_test_df = df5[df5[\"id\"].isin(ids_test)]\n",
    "\n",
    "# Save DataFrames to CSV files including 'id'\n",
    "X_train_df.to_csv('X_train.csv', index=False)\n",
    "X_dev_df.to_csv('X_dev.csv', index=False)\n",
    "X_test_df.to_csv('X_test.csv', index=False)\n",
    "\n",
    "y_train_df.to_csv('y_train.csv', index=False)\n",
    "y_dev_df.to_csv('y_dev.csv', index=False)\n",
    "y_test_df.to_csv('y_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1bcc606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the first row of 'targetTitle_glove':\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the preprocessed DataFrame with GloVe embeddings\n",
    "X_dev_df = pd.read_csv('X_dev.csv')\n",
    "\n",
    "# Extract the 'targetTitle_glove' column as a numpy array\n",
    "targetTitle_glove_values = np.array(X_dev_df['targetTitle_glove'].apply(lambda x: np.fromstring(x[1:-1], sep=' ')).tolist())\n",
    "\n",
    "# Get the value of the first row of 'targetTitle_glove'\n",
    "first_row_value = targetTitle_glove_values[1]\n",
    "\n",
    "print(\"Value of the first row of 'targetTitle_glove':\")\n",
    "print(first_row_value.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817b94ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "220/220 [==============================] - 1s 2ms/step - loss: 0.0416 - val_loss: 0.0295\n",
      "Epoch 2/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0257 - val_loss: 0.0233\n",
      "Epoch 3/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0202\n",
      "Epoch 4/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.0188\n",
      "Epoch 5/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0181 - val_loss: 0.0180\n",
      "Epoch 6/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0173 - val_loss: 0.0175\n",
      "Epoch 7/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 8/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.0166\n",
      "Epoch 9/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.0164\n",
      "Epoch 10/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0157 - val_loss: 0.0161\n",
      "Epoch 11/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0160\n",
      "Epoch 12/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.0159\n",
      "Epoch 13/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.0157\n",
      "Epoch 14/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0157\n",
      "Epoch 15/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0157\n",
      "Epoch 16/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.0156\n",
      "Epoch 17/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0156\n",
      "Epoch 18/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0155\n",
      "Epoch 19/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.0155\n",
      "Epoch 20/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0154\n",
      "Epoch 21/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0153\n",
      "Epoch 22/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0146 - val_loss: 0.0151\n",
      "Epoch 23/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.0150\n",
      "Epoch 24/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.0150\n",
      "Epoch 25/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0150\n",
      "Epoch 26/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0149\n",
      "Epoch 27/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.0149\n",
      "Epoch 28/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.0149\n",
      "Epoch 29/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 0.0149\n",
      "Epoch 30/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0148\n",
      "Epoch 31/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.0148\n",
      "Epoch 32/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0148\n",
      "Epoch 33/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0147\n",
      "Epoch 34/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0145\n",
      "Epoch 35/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0145\n",
      "Epoch 36/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0144\n",
      "Epoch 37/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0142\n",
      "Epoch 38/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0142\n",
      "Epoch 39/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0142\n",
      "Epoch 40/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.0142\n",
      "Epoch 41/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0142\n",
      "Epoch 42/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0141\n",
      "Epoch 43/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.0139\n",
      "Epoch 44/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.0139\n",
      "Epoch 45/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0138\n",
      "Epoch 46/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.0137\n",
      "Epoch 47/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0137\n",
      "Epoch 48/50\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0137\n",
      "Epoch 49/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.0137\n",
      "Epoch 50/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.0137\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 75)                7575      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                3800      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 75)                3825      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               7600      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,000\n",
      "Trainable params: 43,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed DataFrame with GloVe embeddings\n",
    "df5 = pd.read_csv('preprocess_merged_with_glove.csv')\n",
    "\n",
    "# Load the preprocessed 'X_dev' DataFrame with 'targetTitle_glove' column\n",
    "X_dev_df = pd.read_csv('X_dev.csv')\n",
    "X_dev_glove = np.array(X_dev_df['targetTitle_glove'].apply(lambda x: np.fromstring(x[1:-1], sep=' ')).tolist())\n",
    "\n",
    "\n",
    "# Assuming 'targetTitle_glove' has a size of 100 (embedding dimension)\n",
    "input_dim = 100\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "def build_autoencoder(input_dim, bottleneck_dim):\n",
    "    # Encoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(100, activation='relu')(input_layer)\n",
    "    encoded = Dense(75, activation='relu')(encoded)\n",
    "    bottleneck = Dense(bottleneck_dim, activation='relu')(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Dense(75, activation='relu')(bottleneck)\n",
    "    decoded = Dense(100, activation='relu')(decoded)\n",
    "    output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Build the autoencoder\n",
    "bottleneck_dim = 50\n",
    "autoencoder = build_autoencoder(input_dim, bottleneck_dim)\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_dev_glove, X_dev_glove, epochs=50, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Display the summary of the autoencoder model\n",
    "autoencoder.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f26c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 0s 792us/step\n",
      "Encoded representations (compressed embeddings) of X_dev:\n",
      "[[0.08853386 0.11070979 0.         ... 0.02190581 0.00929128 0.        ]\n",
      " [0.03209248 0.01915034 0.0448814  ... 0.         0.03933427 0.        ]\n",
      " [0.         0.08483752 0.         ... 0.         0.0275916  0.        ]\n",
      " ...\n",
      " [0.02287939 0.11505627 0.         ... 0.04448852 0.05177414 0.        ]\n",
      " [0.11031618 0.00932035 0.         ... 0.         0.         0.        ]\n",
      " [0.11285195 0.         0.01888596 ... 0.         0.         0.03435314]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the trained autoencoder model (after the training step)\n",
    "autoencoder = build_autoencoder(input_dim, bottleneck_dim)\n",
    "\n",
    "# Get the encoder part of the autoencoder model\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[-3].output)\n",
    "\n",
    "# Obtain the encoded representations of X_dev\n",
    "encoded_X_dev = encoder.predict(X_dev_glove)\n",
    "\n",
    "print(\"Encoded representations (compressed embeddings) of X_dev:\")\n",
    "print(encoded_X_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca82609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 75)                7575      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 50)                3800      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 75)                3825      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               7600      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,000\n",
      "Trainable params: 43,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "\n",
    "# Save the autoencoder weights to a JSON file\n",
    "autoencoder.save_weights('autoencoder_weights.h5')\n",
    "with open('autoencoder_summary.json', 'w') as f:\n",
    "    json.dump(autoencoder.summary(), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187d7f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 0s 703us/step\n",
      "123/123 [==============================] - 0s 861us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the preprocessed DataFrame with GloVe embeddings and the corresponding IDs\n",
    "X_train_df = pd.read_csv('X_train.csv')\n",
    "X_test_df = pd.read_csv('X_test.csv')\n",
    "\n",
    "# Extract the 'id' column from the DataFrames\n",
    "X_train_ids = X_train_df['id']\n",
    "X_test_ids = X_test_df['id']\n",
    "\n",
    "# Convert 'targetTitle_glove' column into numpy arrays for X_train and X_test\n",
    "X_train_glove = np.array(X_train_df['targetTitle_glove'].apply(lambda x: np.fromstring(x[1:-1], sep=' ')).tolist())\n",
    "X_test_glove = np.array(X_test_df['targetTitle_glove'].apply(lambda x: np.fromstring(x[1:-1], sep=' ')).tolist())\n",
    "\n",
    "\n",
    "# Load the trained autoencoder model (after the training step)\n",
    "autoencoder = build_autoencoder(input_dim + 1, bottleneck_dim)  # Add 1 for the 'class label' column\n",
    "\n",
    "# Get the encoder part of the autoencoder model\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[-3].output)\n",
    "\n",
    "# Concatenate 'targetTitle_glove' with 'class label' columns for X_train and X_test\n",
    "X_train_combined = np.hstack((X_train_glove, np.array(X_train_df['class label']).reshape(-1, 1)))\n",
    "X_test_combined = np.hstack((X_test_glove, np.array(X_test_df['class label']).reshape(-1, 1)))\n",
    "\n",
    "# Obtain the encoded representations of X_train_combined and X_test_combined\n",
    "encoded_X_train_combined = encoder.predict(X_train_combined)\n",
    "encoded_X_test_combined = encoder.predict(X_test_combined)\n",
    "\n",
    "# Extract 50 features from the bottleneck layer for X_train_combined and X_test_combined\n",
    "extracted_features_X_train = encoded_X_train_combined[:, :50]\n",
    "extracted_features_X_test = encoded_X_test_combined[:, :50]\n",
    "\n",
    "# Create DataFrames for extracted features including 'id' column\n",
    "X_train_features_df = pd.DataFrame(data=np.hstack((X_train_ids.values.reshape(-1, 1), extracted_features_X_train)),\n",
    "                                   columns=['id'] + [f'feature_{i+1}' for i in range(50)])\n",
    "\n",
    "X_test_features_df = pd.DataFrame(data=np.hstack((X_test_ids.values.reshape(-1, 1), extracted_features_X_test)),\n",
    "                                  columns=['id'] + [f'feature_{i+1}' for i in range(50)])\n",
    "\n",
    "# Save the extracted features to CSV files\n",
    "X_train_features_df.to_csv('X_train_features.csv', index=False)\n",
    "X_test_features_df.to_csv('X_test_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "337da407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the datasets\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_train_features = pd.read_csv('X_train_features.csv')\n",
    "\n",
    "# Select the specified columns from X_train DataFrame\n",
    "selected_columns = ['sim between postText and Title',\n",
    "                    'Pt_Readability of postText',\n",
    "                    'Paragraph Ratio of formal and informal word',\n",
    "                    'entroy',\n",
    "                    'senti_score_absolute',\n",
    "                    'lexical Diversity',\n",
    "                    'title variance',                                                    \n",
    "                    'postText variance', \n",
    "                    'Pt_Readability of postText',\n",
    "                    'Pt_POS 2-gram NNP',\n",
    "                    'Pt_Number of DT',\n",
    "                    'Pt_Readability of postText',\n",
    "                    'Number of NNP',\n",
    "                    'POS 2-gram NNP NNP',\n",
    "                    'paragraph Readability',\n",
    "                    'readability of title',\n",
    "                    'paragraphs Number of NNP',\n",
    "                    'lexical Diversity',\n",
    "                    'title variance',\n",
    "                    'postText variance',\n",
    "                    'class label',\n",
    "                    'id']\n",
    "selected_features = X_train[selected_columns]\n",
    "\n",
    "# Merge the selected features with X_train_features DataFrame based on 'id'\n",
    "X_train_final = pd.merge(X_train_features, selected_features, on='id')\n",
    "\n",
    "#Save the merged DataFrame as a new CSV file\n",
    "X_train_final.to_csv('X_train_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ec1874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the datasets\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "X_test_features = pd.read_csv('X_test_features.csv')\n",
    "\n",
    "#Select the specified columns from X_test DataFrame\n",
    "selected_columns = ['sim between postText and Title',\n",
    "                    'Pt_Readability of postText',\n",
    "                    'Paragraph Ratio of formal and informal word',\n",
    "                    'entroy',\n",
    "                    'senti_score_absolute',\n",
    "                    'lexical Diversity',\n",
    "                    'title variance',                                                    \n",
    "                    'postText variance', \n",
    "                    'Pt_Readability of postText',\n",
    "                    'Pt_POS 2-gram NNP',\n",
    "                    'Pt_Number of DT',\n",
    "                    'Pt_Readability of postText',\n",
    "                    'Number of NNP',\n",
    "                    'POS 2-gram NNP NNP',\n",
    "                    'paragraph Readability',\n",
    "                    'readability of title',\n",
    "                    'paragraphs Number of NNP',\n",
    "                    'lexical Diversity',\n",
    "                    'title variance',\n",
    "                    'postText variance',\n",
    "                    'class label',\n",
    "                    'id']\n",
    "selected_features = X_test[selected_columns]\n",
    "\n",
    "#Merge the selected features with X_test_features DataFrame based on 'id'\n",
    "X_test_final = pd.merge(X_test_features, selected_features, on='id')\n",
    "\n",
    "#Save the merged DataFrame as a new CSV file\n",
    "X_test_final.to_csv('X_test_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08eb5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training and testing datasets\n",
    "X_train_final = pd.read_csv('X_train_final.csv')\n",
    "X_test_final = pd.read_csv('X_test_final.csv')\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "y_train = X_train_final['class label']\n",
    "y_test = X_test_final['class label']\n",
    "col_drop=['id', 'class label']\n",
    "X_train = X_train_final.drop(col_drop, axis=1)\n",
    "X_test = X_test_final.drop(col_drop, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d47a39a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM...\n",
      "SVM Accuracy: 0.8593\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91      1801\n",
      "           1       0.86      0.51      0.64       587\n",
      "\n",
      "    accuracy                           0.86      2388\n",
      "   macro avg       0.86      0.74      0.78      2388\n",
      "weighted avg       0.86      0.86      0.85      2388\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[1753   48]\n",
      " [ 288  299]]\n",
      "==================================================\n",
      "Training Random Forest...\n",
      "Random Forest Accuracy: 0.8509\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91      1801\n",
      "           1       0.86      0.47      0.61       587\n",
      "\n",
      "    accuracy                           0.85      2388\n",
      "   macro avg       0.86      0.72      0.76      2388\n",
      "weighted avg       0.85      0.85      0.83      2388\n",
      "\n",
      "Random Forest Confusion Matrix:\n",
      "[[1757   44]\n",
      " [ 312  275]]\n",
      "==================================================\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.8593\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91      1801\n",
      "           1       0.83      0.53      0.65       587\n",
      "\n",
      "    accuracy                           0.86      2388\n",
      "   macro avg       0.85      0.75      0.78      2388\n",
      "weighted avg       0.86      0.86      0.85      2388\n",
      "\n",
      "Logistic Regression Confusion Matrix:\n",
      "[[1738   63]\n",
      " [ 273  314]]\n",
      "==================================================\n",
      "Training KNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nites\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.8224\n",
      "KNN Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89      1801\n",
      "           1       0.73      0.44      0.55       587\n",
      "\n",
      "    accuracy                           0.82      2388\n",
      "   macro avg       0.78      0.69      0.72      2388\n",
      "weighted avg       0.81      0.82      0.81      2388\n",
      "\n",
      "KNN Confusion Matrix:\n",
      "[[1705   96]\n",
      " [ 328  259]]\n",
      "==================================================\n",
      "Training Naive Bayes...\n",
      "Naive Bayes Accuracy: 0.4192\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.24      0.39      1801\n",
      "           1       0.29      0.95      0.45       587\n",
      "\n",
      "    accuracy                           0.42      2388\n",
      "   macro avg       0.62      0.60      0.42      2388\n",
      "weighted avg       0.78      0.42      0.40      2388\n",
      "\n",
      "Naive Bayes Confusion Matrix:\n",
      "[[ 441 1360]\n",
      " [  27  560]]\n",
      "==================================================\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 0.8790\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92      1801\n",
      "           1       0.84      0.63      0.72       587\n",
      "\n",
      "    accuracy                           0.88      2388\n",
      "   macro avg       0.86      0.79      0.82      2388\n",
      "weighted avg       0.88      0.88      0.87      2388\n",
      "\n",
      "Gradient Boosting Confusion Matrix:\n",
      "[[1732   69]\n",
      " [ 220  367]]\n",
      "==================================================\n",
      "Training Decision Tree...\n",
      "Decision Tree Accuracy: 0.7747\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      1801\n",
      "           1       0.54      0.53      0.54       587\n",
      "\n",
      "    accuracy                           0.77      2388\n",
      "   macro avg       0.70      0.69      0.69      2388\n",
      "weighted avg       0.77      0.77      0.77      2388\n",
      "\n",
      "Decision Tree Confusion Matrix:\n",
      "[[1536  265]\n",
      " [ 273  314]]\n",
      "==================================================\n",
      "Training AdaBoost...\n",
      "AdaBoost Accuracy: 0.8643\n",
      "AdaBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91      1801\n",
      "           1       0.76      0.66      0.70       587\n",
      "\n",
      "    accuracy                           0.86      2388\n",
      "   macro avg       0.83      0.80      0.81      2388\n",
      "weighted avg       0.86      0.86      0.86      2388\n",
      "\n",
      "AdaBoost Confusion Matrix:\n",
      "[[1677  124]\n",
      " [ 200  387]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize classifiers\n",
    "svm_classifier = SVC()\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "logistic_regression_classifier = LogisticRegression()\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "naive_bayes_classifier = GaussianNB()\n",
    "gradient_boosting_classifier = GradientBoostingClassifier()\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "adaboost_classifier = AdaBoostClassifier()\n",
    "\n",
    "# Train and predict with each classifier\n",
    "classifiers = [\n",
    "    ('SVM', svm_classifier),\n",
    "    ('Random Forest', random_forest_classifier),\n",
    "    ('Logistic Regression', logistic_regression_classifier),\n",
    "    ('KNN', knn_classifier),\n",
    "    ('Naive Bayes', naive_bayes_classifier),\n",
    "    ('Gradient Boosting', gradient_boosting_classifier),\n",
    "    ('Decision Tree', decision_tree_classifier),\n",
    "    ('AdaBoost', adaboost_classifier)\n",
    "]\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    print(f\"Training {name}...\")\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "    print(f\"{name} Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"{name} Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8a03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
